{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOtUFbiFHz4ywz3VEAbyUBQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/23520052/DoAnDuLieuLon/blob/main/2_Baseline_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i-DC6wpWbN8"
      },
      "outputs": [],
      "source": [
        "# --- PH·∫¶N 1: C√ÄI ƒê·∫∂T & K·∫æT N·ªêI ---\n",
        "# 1. C√†i th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "!pip install -q transformers torch scikit-learn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "# 2. Ki·ªÉm tra GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ ƒê√É B·∫¨T GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    raise SystemError(\"‚ùå L·ªñI: Ch∆∞a b·∫≠t GPU! H√£y v√†o Runtime -> Change runtime type -> T4 GPU\")\n",
        "\n",
        "# 3. K·∫øt n·ªëi Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- S·ª¨A ƒê∆Ø·ªúNG D·∫™N D∆Ø·ªöI ƒê√ÇY CHO ƒê√öNG TH∆Ø M·ª§C C·ª¶A B·∫†N ---\n",
        "folder_path = '/content/drive/My Drive/ABSA_Project'\n",
        "\n",
        "try:\n",
        "    os.chdir(folder_path)\n",
        "    print(f\"‚úÖ ƒêang l√†m vi·ªác t·∫°i th∆∞ m·ª•c: {os.getcwd()}\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c! H√£y ki·ªÉm tra l·∫°i bi·∫øn 'folder_path'.\")\n",
        "\n",
        "# --- PH·∫¶N 2: X·ª¨ L√ù D·ªÆ LI·ªÜU (Parser) ---\n",
        "def parse_vlsp_data(file_name):\n",
        "    \"\"\"ƒê·ªçc file txt ƒë·ªãnh d·∫°ng VLSP 2018\"\"\"\n",
        "    lines = []\n",
        "    try:\n",
        "        # Th·ª≠ ƒë·ªçc utf-16\n",
        "        with open(file_name, 'r', encoding='utf-16') as f:\n",
        "            lines = f.readlines()\n",
        "    except:\n",
        "        try:\n",
        "            # N·∫øu l·ªói th√¨ ƒë·ªçc utf-8\n",
        "            with open(file_name, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "        except:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    dataset = []\n",
        "    review_buffer = []\n",
        "    # Regex t√¨m nh√£n: {ASPECT, SENTIMENT}\n",
        "    label_pattern = re.compile(r\"\\{(.*?),\\s*(.*?)\\}\")\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        if \"{\" in line and \"}\" in line and \"#\" in line:\n",
        "            current_review = \" \".join(review_buffer).strip()\n",
        "            matches = label_pattern.findall(line)\n",
        "            for aspect, sentiment in matches:\n",
        "                if sentiment.lower() != 'none': # B·ªè qua nh√£n None\n",
        "                    dataset.append({\n",
        "                        'text': current_review,\n",
        "                        'aspect': aspect.strip(),\n",
        "                        'label': sentiment.strip().lower()\n",
        "                    })\n",
        "            review_buffer = []\n",
        "        else:\n",
        "            review_buffer.append(re.sub(r\"^[\\_\\-]\\s*\", \"\", line))\n",
        "    return pd.DataFrame(dataset)\n",
        "\n",
        "# ƒê·ªçc d·ªØ li·ªáu\n",
        "print(\"‚è≥ ƒêang t·∫£i d·ªØ li·ªáu...\")\n",
        "df_train = parse_vlsp_data('1-VLSP2018-SA-Restaurant-train.txt')\n",
        "df_test = parse_vlsp_data('3-VLSP2018-SA-Restaurant-test.txt')\n",
        "print(f\"‚úÖ Train set: {len(df_train)} d√≤ng | Test set: {len(df_test)} d√≤ng\")\n",
        "\n",
        "# --- PH·∫¶N 3: CHU·∫®N B·ªä DATASET CHO PHOBERT ---\n",
        "print(\"‚è≥ ƒêang t·∫£i Tokenizer PhoBERT...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=128):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        # Input format: Aspect [SEP] Review Text\n",
        "        input_text = f\"{row['aspect']} {self.tokenizer.sep_token} {row['text']}\"\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.label_map[row['label']], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# T·∫°o DataLoader\n",
        "BATCH_SIZE = 32 # TƒÉng l√™n 32 v√¨ GPU T4 ƒë·ªß m·∫°nh\n",
        "train_loader = DataLoader(SentimentDataset(df_train, tokenizer), batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(SentimentDataset(df_test, tokenizer), batch_size=BATCH_SIZE)\n",
        "\n",
        "# --- PH·∫¶N 4: HU·∫§N LUY·ªÜN (TRAINING) ---\n",
        "print(\"‚è≥ ƒêang t·∫£i m√¥ h√¨nh PhoBERT (Pre-trained)...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=3)\n",
        "model.to(device)\n",
        "\n",
        "# C·∫•u h√¨nh\n",
        "EPOCHS = 3\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "print(f\"\\nüöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN TR√äN GPU {torch.cuda.get_device_name(0)}...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        # ƒê·∫©y d·ªØ li·ªáu v√†o GPU\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # X·ª≠ l√Ω\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if step % 50 == 0 and step > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"  Epoch {epoch+1} | Batch {step}/{len(train_loader)} | Loss: {loss.item():.4f} | Time: {elapsed:.0f}s\")\n",
        "\n",
        "# --- PH·∫¶N 5: ƒê√ÅNH GI√Å (EVALUATION) ---\n",
        "print(\"\\n‚úÖ ƒê√£ train xong! ƒêang ch·∫•m ƒëi·ªÉm tr√™n t·∫≠p Test...\")\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# IN B√ÅO C√ÅO CU·ªêI C√ôNG\n",
        "print(\"\\nüìä B·∫¢NG K·∫æT QU·∫¢ CHI TI·∫æT (BASELINE - CH∆ØA C√ÇN B·∫∞NG):\")\n",
        "target_names = ['Negative', 'Neutral', 'Positive']\n",
        "print(classification_report(true_labels, predictions, target_names=target_names, digits=4, zero_division=0))"
      ]
    }
  ]
}