{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxwXbJ09LPUIJt/Uv9rXix",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/23520052/DoAnDuLieuLon/blob/main/3_Final_Model_with_BackTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HU·∫§N LUY·ªÜN TR√äN D·ªÆ LI·ªÜU ƒê√É C√ÇN B·∫∞NG**"
      ],
      "metadata": {
        "id": "UO9eNuNBUH5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- B∆Ø·ªöC 1: C√ÄI ƒê·∫∂T & K·∫æT N·ªêI ---\n",
        "!pip install -q transformers torch scikit-learn\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# S·ª¨A L·ªñI: B·ªè AdamW ·ªü d√≤ng n√†y\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "# S·ª¨A L·ªñI: Import AdamW t·ª´ torch.optim\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import classification_report\n",
        "from google.colab import drive\n",
        "\n",
        "# K·∫øt n·ªëi Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# S·ª≠a ƒë∆∞·ªùng d·∫´n folder_path n·∫øu c·∫ßn\n",
        "folder_path = '/content/drive/My Drive/ABSA_Project'\n",
        "try:\n",
        "    os.chdir(folder_path)\n",
        "    print(f\"‚úÖ ƒêang l√†m vi·ªác t·∫°i: {os.getcwd()}\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n folder_path!\")\n",
        "\n",
        "# Ki·ªÉm tra GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üöÄ Thi·∫øt b·ªã: {device}\")\n",
        "\n",
        "# --- B∆Ø·ªöC 2: H√ÄM ƒê·ªåC D·ªÆ LI·ªÜU ---\n",
        "def parse_vlsp_data(file_name):\n",
        "    lines = []\n",
        "    try:\n",
        "        with open(file_name, 'r', encoding='utf-16') as f:\n",
        "            lines = f.readlines()\n",
        "    except:\n",
        "        try:\n",
        "            with open(file_name, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "        except:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    dataset = []\n",
        "    review_buffer = []\n",
        "    label_pattern = re.compile(r\"\\{(.*?),\\s*(.*?)\\}\")\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        if \"{\" in line and \"}\" in line and \"#\" in line:\n",
        "            current_review = \" \".join(review_buffer).strip()\n",
        "            matches = label_pattern.findall(line)\n",
        "            for aspect, sentiment in matches:\n",
        "                if sentiment.lower() != 'none':\n",
        "                    dataset.append({'text': current_review, 'aspect': aspect.strip(), 'label': sentiment.strip().lower()})\n",
        "            review_buffer = []\n",
        "        else:\n",
        "            review_buffer.append(re.sub(r\"^[\\_\\-]\\s*\", \"\", line))\n",
        "    return pd.DataFrame(dataset)\n",
        "\n",
        "# --- B∆Ø·ªöC 3: LOAD D·ªÆ LI·ªÜU ---\n",
        "print(\"\\n‚è≥ ƒêang t·∫£i d·ªØ li·ªáu...\")\n",
        "\n",
        "# 1. Load t·∫≠p Train ƒë√£ C√¢n b·∫±ng (CSV)\n",
        "try:\n",
        "    df_train_balanced = pd.read_csv('1-VLSP2018-Restaurant-Train-BALANCED.csv')\n",
        "    print(f\"‚úÖ ƒê√£ t·∫£i t·∫≠p TRAIN (Balanced): {len(df_train_balanced)} d√≤ng\")\n",
        "    # Ki·ªÉm tra ph√¢n b·ªë\n",
        "    print(\"   Ph√¢n b·ªë:\", df_train_balanced['label'].value_counts().to_dict())\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file 'BALANCED.csv'. B·∫°n ki·ªÉm tra l·∫°i t√™n file trong Drive nh√©!\")\n",
        "\n",
        "# 2. Load t·∫≠p Test g·ªëc (TXT)\n",
        "df_test = parse_vlsp_data('3-VLSP2018-SA-Restaurant-test.txt')\n",
        "print(f\"‚úÖ ƒê√£ t·∫£i t·∫≠p TEST (G·ªëc): {len(df_test)} d√≤ng\")\n",
        "\n",
        "# --- B∆Ø·ªöC 4: TOKENIZER & DATALOADER ---\n",
        "print(\"\\n‚è≥ ƒêang thi·∫øt l·∫≠p PhoBERT Tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=128):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        input_text = f\"{str(row['aspect'])} {self.tokenizer.sep_token} {str(row['text'])}\"\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            input_text, add_special_tokens=True, max_length=self.max_len,\n",
        "            padding='max_length', truncation=True, return_attention_mask=True, return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.label_map[row['label']], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# T·∫°o DataLoader\n",
        "BATCH_SIZE = 32\n",
        "train_loader_balanced = DataLoader(SentimentDataset(df_train_balanced, tokenizer), batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(SentimentDataset(df_test, tokenizer), batch_size=BATCH_SIZE)\n",
        "\n",
        "print(\"\\nüéâ SETUP XONG! S·∫µn s√†ng hu·∫•n luy·ªán.\")"
      ],
      "metadata": {
        "id": "kRnGGcIfUJjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- HU·∫§N LUY·ªÜN TR√äN D·ªÆ LI·ªÜU C√ÇN B·∫∞NG ---\n",
        "print(\"‚è≥ ƒêang t·∫£i l·∫°i m√¥ h√¨nh PhoBERT s·∫°ch (Reset)...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=3)\n",
        "model.to(device)\n",
        "\n",
        "# C·∫•u h√¨nh\n",
        "EPOCHS = 3\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5) # ƒê√£ d√πng ƒë√∫ng AdamW t·ª´ torch\n",
        "total_steps = len(train_loader_balanced) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "print(f\"\\nüöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN V·ªöI D·ªÆ LI·ªÜU C√ÇN B·∫∞NG (Balanced Training)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_loader_balanced):\n",
        "        # ƒê·∫©y v√†o GPU\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # In ti·∫øn ƒë·ªô\n",
        "        if step % 100 == 0 and step > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"  Epoch {epoch+1} | Batch {step}/{len(train_loader_balanced)} | Loss: {loss.item():.4f} | Time: {elapsed:.0f}s\")\n",
        "\n",
        "# --- ƒê√ÅNH GI√Å K·∫æT QU·∫¢ ---\n",
        "print(\"\\n‚úÖ Train xong! ƒêang ch·∫•m ƒëi·ªÉm l·∫°i tr√™n t·∫≠p Test...\")\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# IN B√ÅO C√ÅO K·∫æT QU·∫¢\n",
        "print(\"\\nüìä B·∫¢NG K·∫æT QU·∫¢ CHI TI·∫æT (SAU KHI C√ÇN B·∫∞NG D·ªÆ LI·ªÜU):\")\n",
        "target_names = ['Negative', 'Neutral', 'Positive']\n",
        "print(classification_report(true_labels, predictions, target_names=target_names, digits=4, zero_division=0))"
      ],
      "metadata": {
        "id": "qcwPDEFDXRpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load l·∫°i file ƒë√£ c√¢n b·∫±ng c≈©\n",
        "try:\n",
        "    df_check = pd.read_csv('1-VLSP2018-Restaurant-Train-BALANCED.csv')\n",
        "\n",
        "    # L·∫•y c√°c m·∫´u Negative ƒë∆∞·ª£c sinh th√™m (th∆∞·ªùng n·∫±m ·ªü cu·ªëi file)\n",
        "    # Gi·∫£ s·ª≠ 9000 d√≤ng ƒë·∫ßu l√† g·ªëc, sau ƒë√≥ l√† sinh th√™m\n",
        "    generated_samples = df_check.iloc[9300:].sample(10)\n",
        "\n",
        "    print(\"--- KI·ªÇM TRA CH·∫§T L∆Ø·ª¢NG D·ªÆ LI·ªÜU SINH RA (EDA) ---\")\n",
        "    for idx, row in generated_samples.iterrows():\n",
        "        print(f\"Label: {row['label']}\")\n",
        "        print(f\"Cau: {row['text']}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "except:\n",
        "    print(\"B·∫°n c·∫ßn ch·∫°y l·∫°i b∆∞·ªõc load file CSV tr∆∞·ªõc.\")"
      ],
      "metadata": {
        "id": "jephW0bAqdxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- B∆Ø·ªöC 1: C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN D·ªäCH ---\n",
        "!pip install -q deep-translator\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# --- B∆Ø·ªöC 2: H√ÄM D·ªäCH NG∆Ø·ª¢C (BACK-TRANSLATION) ---\n",
        "def back_translate(sentence):\n",
        "    try:\n",
        "        # 1. Vi·ªát -> Anh\n",
        "        en_text = GoogleTranslator(source='vi', target='en').translate(sentence)\n",
        "        # 2. Anh -> Vi·ªát\n",
        "        vi_text = GoogleTranslator(source='en', target='vi').translate(en_text)\n",
        "        return vi_text\n",
        "    except:\n",
        "        # N·∫øu l·ªói m·∫°ng th√¨ tr·∫£ v·ªÅ c√¢u g·ªëc\n",
        "        return sentence\n",
        "\n",
        "# Test th·ª≠ xem n√≥ th√¥ng minh th·∫ø n√†o\n",
        "sample = \"M√≥n ƒÉn ·ªü ƒë√¢y kh√¥ng ngon ch√∫t n√†o, th√°i ƒë·ªô nh√¢n vi√™n l·ªìi l√µm.\"\n",
        "print(f\"G·ªëc: {sample}\")\n",
        "print(f\"M·ªõi: {back_translate(sample)}\")\n",
        "\n",
        "# --- B∆Ø·ªöC 3: QUY TR√åNH C√ÇN B·∫∞NG D·ªÆ LI·ªÜU (K·∫øt h·ª£p Oversampling + Back-Translation) ---\n",
        "def balance_with_back_translation(df):\n",
        "    print(\"üöÄ ƒêang th·ª±c hi·ªán TƒÉng c∆∞·ªùng d·ªØ li·ªáu b·∫±ng D·ªäCH NG∆Ø·ª¢C (Ch·∫•t l∆∞·ª£ng cao)...\")\n",
        "\n",
        "    # T√°ch c√°c l·ªõp\n",
        "    df_pos = df[df['label'] == 'positive']\n",
        "    df_neu = df[df['label'] == 'neutral']\n",
        "    df_neg = df[df['label'] == 'negative']\n",
        "\n",
        "    # L·∫•y s·ªë l∆∞·ª£ng target (Positive)\n",
        "    max_count = len(df_pos)\n",
        "\n",
        "    # Danh s√°ch ch·ª©a d·ªØ li·ªáu cu·ªëi c√πng (B·∫Øt ƒë·∫ßu b·∫±ng d·ªØ li·ªáu g·ªëc)\n",
        "    final_data = df.to_dict('records')\n",
        "\n",
        "    # --- X·ª¨ L√ù L·ªöP NEGATIVE (Quan tr·ªçng nh·∫•t) ---\n",
        "    # V√¨ Negative r·∫•t √≠t (670 m·∫´u), ta c·∫ßn sinh th√™m r·∫•t nhi·ªÅu\n",
        "    neg_needed = max_count - len(df_neg)\n",
        "    print(f\"   + ƒêang sinh th√™m {neg_needed} m·∫´u Negative...\")\n",
        "\n",
        "    neg_samples = df_neg.to_dict('records')\n",
        "    for _ in tqdm(range(neg_needed)):\n",
        "        # Ch·ªçn ng·∫´u nhi√™n 1 c√¢u m·∫´u\n",
        "        sample = random.choice(neg_samples)\n",
        "\n",
        "        # Chi·∫øn thu·∫≠t: 50% d√πng D·ªãch ng∆∞·ª£c (T·∫°o m·ªõi), 50% d√πng Copy (An to√†n)\n",
        "        # L√Ω do: D·ªãch nhi·ªÅu qu√° s·∫Ω ch·∫≠m, k·∫øt h·ª£p copy ƒë·ªÉ nhanh h∆°n\n",
        "        if random.random() < 0.5:\n",
        "            new_text = back_translate(sample['text'])\n",
        "        else:\n",
        "            new_text = sample['text'] # Copy nguy√™n b·∫£n\n",
        "\n",
        "        final_data.append({\n",
        "            'text': new_text,\n",
        "            'aspect': sample['aspect'],\n",
        "            'label': sample['label']\n",
        "        })\n",
        "\n",
        "    # --- X·ª¨ L√ù L·ªöP NEUTRAL ---\n",
        "    neu_needed = max_count - len(df_neu)\n",
        "    print(f\"   + ƒêang sinh th√™m {neu_needed} m·∫´u Neutral...\")\n",
        "\n",
        "    neu_samples = df_neu.to_dict('records')\n",
        "    for _ in tqdm(range(neu_needed)):\n",
        "        sample = random.choice(neu_samples)\n",
        "        # V·ªõi Neutral, ta d√πng Copy nhi·ªÅu h∆°n ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian (70% Copy)\n",
        "        if random.random() < 0.3:\n",
        "            new_text = back_translate(sample['text'])\n",
        "        else:\n",
        "            new_text = sample['text']\n",
        "\n",
        "        final_data.append({\n",
        "            'text': new_text,\n",
        "            'aspect': sample['aspect'],\n",
        "            'label': sample['label']\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(final_data)\n",
        "\n",
        "# --- TH·ª∞C THI (L∆∞u √Ω: S·∫Ω m·∫•t kho·∫£ng 10-15 ph√∫t v√¨ ph·∫£i d·ªãch online) ---\n",
        "# Load l·∫°i d·ªØ li·ªáu g·ªëc\n",
        "df_train_original = parse_vlsp_data('1-VLSP2018-SA-Restaurant-train.txt')\n",
        "\n",
        "# Ch·∫°y c√¢n b·∫±ng\n",
        "df_train_bt = balance_with_back_translation(df_train_original)\n",
        "\n",
        "# L∆∞u file x·ªãn nh·∫•t\n",
        "df_train_bt.to_csv('1-VLSP2018-Restaurant-Train-BACKTRANS.csv', index=False)\n",
        "print(\"üíæ ƒê√£ l∆∞u file ch·∫•t l∆∞·ª£ng cao: 1-VLSP2018-Restaurant-Train-BACKTRANS.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBffK_NRrfEI",
        "outputId": "6e75be5d-65c0-47b5-bc15-efe67d93d895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|‚ñè         | 90/6401 [01:30<1:29:13,  1.18it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForSequenceClassification, get_linear_schedule_with_warmup, AutoTokenizer\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# --- B∆Ø·ªöC 1: LOAD D·ªÆ LI·ªÜU ƒê√É D·ªäCH NG∆Ø·ª¢C ---\n",
        "print(\"‚è≥ ƒêang t·∫£i d·ªØ li·ªáu Back-Translation...\")\n",
        "try:\n",
        "    # ƒê·ªçc file b·∫°n v·ª´a t·∫°o xong\n",
        "    df_train_final = pd.read_csv('1-VLSP2018-Restaurant-Train-BACKTRANS.csv')\n",
        "    print(f\"‚úÖ ƒê√£ t·∫£i t·∫≠p TRAIN (Final): {len(df_train_final)} d√≤ng\")\n",
        "    print(\"   Ph√¢n b·ªë:\", df_train_final['label'].value_counts().to_dict())\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file csv. H√£y ki·ªÉm tra l·∫°i t√™n file.\")\n",
        "\n",
        "# Load l·∫°i t·∫≠p Test (n·∫øu ch∆∞a c√≥)\n",
        "if 'df_test' not in locals():\n",
        "    # Gi·∫£ ƒë·ªãnh h√†m parse_vlsp_data ƒë√£ c√≥ t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc\n",
        "    df_test = parse_vlsp_data('3-VLSP2018-SA-Restaurant-test.txt')\n",
        "\n",
        "# --- B∆Ø·ªöC 2: CHU·∫®N B·ªä DATALOADER ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "# T·∫°o DataLoader cho t·∫≠p train m·ªõi\n",
        "train_loader_final = DataLoader(SentimentDataset(df_train_final, tokenizer), batch_size=32, shuffle=True)\n",
        "# DataLoader cho t·∫≠p test (gi·ªØ nguy√™n)\n",
        "test_loader = DataLoader(SentimentDataset(df_test, tokenizer), batch_size=32)\n",
        "\n",
        "# --- B∆Ø·ªöC 3: KH·ªûI T·∫†O M√î H√åNH (RESET) ---\n",
        "print(\"\\nüßπ Reset m√¥ h√¨nh ƒë·ªÉ h·ªçc l·∫°i t·ª´ ƒë·∫ßu...\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=3)\n",
        "model.to(device)\n",
        "\n",
        "# --- B∆Ø·ªöC 4: HU·∫§N LUY·ªÜN (TRAINING LOOP) ---\n",
        "EPOCHS = 3\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_loader_final) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "print(f\"\\nüöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN (PH∆Ø∆†NG PH√ÅP BACK-TRANSLATION)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_loader_final):\n",
        "        # ƒê·∫©y v√†o GPU\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # In ti·∫øn ƒë·ªô (M·ªói 100 batch)\n",
        "        if step % 100 == 0 and step > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"  Epoch {epoch+1} | Batch {step}/{len(train_loader_final)} | Loss: {loss.item():.4f} | Time: {elapsed:.0f}s\")\n",
        "\n",
        "# --- B∆Ø·ªöC 5: ƒê√ÅNH GI√Å K·∫æT QU·∫¢ CU·ªêI C√ôNG ---\n",
        "print(\"\\n‚úÖ Train xong! ƒêang ch·∫•m ƒëi·ªÉm l·∫°i tr√™n t·∫≠p Test...\")\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# IN B√ÅO C√ÅO K·∫æT QU·∫¢\n",
        "print(\"\\nüìä B·∫¢NG K·∫æT QU·∫¢ CHI TI·∫æT (FINAL - BACK TRANSLATION):\")\n",
        "target_names = ['Negative', 'Neutral', 'Positive']\n",
        "print(classification_report(true_labels, predictions, target_names=target_names, digits=4, zero_division=0))"
      ],
      "metadata": {
        "id": "lg7WFtQO4yXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# S·ªë li·ªáu t·ª´ c√°c l·∫ßn ch·∫°y c·ªßa b·∫°n (Nh·∫≠p tay t·ª´ c√°c b·∫£ng classification_report)\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "# F1-Score c·ªßa Baseline (D·ªØ li·ªáu g·ªëc) - L·∫•y t·ª´ ·∫£nh c≈© c·ªßa b·∫°n\n",
        "baseline_f1 = [0.39, 0.49, 0.85]\n",
        "\n",
        "# F1-Score c·ªßa Final Model (Back-Translation) - L·∫•y t·ª´ k·∫øt qu·∫£ v·ª´a ch·∫°y\n",
        "final_f1 = [0.41, 0.57, 0.85]\n",
        "\n",
        "x = np.arange(len(labels))  # V·ªã tr√≠ c√°c nh√£n\n",
        "width = 0.35  # ƒê·ªô r·ªông c·ªôt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# V·∫Ω 2 c·ªôt\n",
        "rects1 = ax.bar(x - width/2, baseline_f1, width, label='Baseline (Imbalanced)', color='#95a5a6')\n",
        "rects2 = ax.bar(x + width/2, final_f1, width, label='Back-Translation (Balanced)', color='#2ecc71')\n",
        "\n",
        "# Trang tr√≠\n",
        "ax.set_ylabel('F1-Score')\n",
        "ax.set_title('HI·ªÜU QU·∫¢ C·ª¶A PH∆Ø∆†NG PH√ÅP TƒÇNG C∆Ø·ªúNG D·ªÆ LI·ªÜU (BACK-TRANSLATION)', fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1.1) # Gi·ªõi h·∫°n tr·ª•c y\n",
        "\n",
        "# H√†m hi·ªÉn th·ªã s·ªë tr√™n ƒë·∫ßu c·ªôt\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FjtrScWd-z3w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}