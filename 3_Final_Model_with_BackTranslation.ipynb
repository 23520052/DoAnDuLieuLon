{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHRQnDQCm8Xj9qn7ZY2KVP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/23520052/DoAnDuLieuLon/blob/main/3_Final_Model_with_BackTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HUẤN LUYỆN TRÊN DỮ LIỆU ĐÃ CÂN BẰNG**"
      ],
      "metadata": {
        "id": "UO9eNuNBUH5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BƯỚC 1: CÀI ĐẶT & KẾT NỐI ---\n",
        "!pip install -q transformers torch scikit-learn\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# SỬA LỖI: Bỏ AdamW ở dòng này\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "# SỬA LỖI: Import AdamW từ torch.optim\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import classification_report\n",
        "from google.colab import drive\n",
        "\n",
        "# Kết nối Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Sửa đường dẫn folder_path nếu cần\n",
        "folder_path = '/content/drive/My Drive/ABSA_Project'\n",
        "try:\n",
        "    os.chdir(folder_path)\n",
        "    print(f\"Đang làm việc tại: {os.getcwd()}\")\n",
        "except:\n",
        "    print(\"Kiểm tra lại đường dẫn folder_path!\")\n",
        "\n",
        "# Kiểm tra GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Thiết bị: {device}\")\n",
        "\n",
        "# --- BƯỚC 2: HÀM ĐỌC DỮ LIỆU ---\n",
        "def parse_vlsp_data(file_name):\n",
        "    lines = []\n",
        "    try:\n",
        "        with open(file_name, 'r', encoding='utf-16') as f:\n",
        "            lines = f.readlines()\n",
        "    except:\n",
        "        try:\n",
        "            with open(file_name, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "        except:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    dataset = []\n",
        "    review_buffer = []\n",
        "    label_pattern = re.compile(r\"\\{(.*?),\\s*(.*?)\\}\")\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        if \"{\" in line and \"}\" in line and \"#\" in line:\n",
        "            current_review = \" \".join(review_buffer).strip()\n",
        "            matches = label_pattern.findall(line)\n",
        "            for aspect, sentiment in matches:\n",
        "                if sentiment.lower() != 'none':\n",
        "                    dataset.append({'text': current_review, 'aspect': aspect.strip(), 'label': sentiment.strip().lower()})\n",
        "            review_buffer = []\n",
        "        else:\n",
        "            review_buffer.append(re.sub(r\"^[\\_\\-]\\s*\", \"\", line))\n",
        "    return pd.DataFrame(dataset)\n",
        "\n",
        "# --- BƯỚC 3: LOAD DỮ LIỆU ---\n",
        "print(\"\\n⏳ Đang tải dữ liệu...\")\n",
        "\n",
        "# 1. Load tập Train đã Cân bằng (CSV)\n",
        "try:\n",
        "    df_train_balanced = pd.read_csv('1-VLSP2018-Restaurant-Train-BALANCED.csv')\n",
        "    print(f\"Đã tải tập TRAIN (Balanced): {len(df_train_balanced)} dòng\")\n",
        "    # Kiểm tra phân bố\n",
        "    print(\"   Phân bố:\", df_train_balanced['label'].value_counts().to_dict())\n",
        "except FileNotFoundError:\n",
        "    print(\"LỖI: Không tìm thấy file 'BALANCED.csv'. Bạn kiểm tra lại tên file trong Drive nhé!\")\n",
        "\n",
        "# 2. Load tập Test gốc (TXT)\n",
        "df_test = parse_vlsp_data('3-VLSP2018-SA-Restaurant-test.txt')\n",
        "print(f\"Đã tải tập TEST (Gốc): {len(df_test)} dòng\")\n",
        "\n",
        "# --- BƯỚC 4: TOKENIZER & DATALOADER ---\n",
        "print(\"\\nĐang thiết lập PhoBERT Tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=128):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        input_text = f\"{str(row['aspect'])} {self.tokenizer.sep_token} {str(row['text'])}\"\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            input_text, add_special_tokens=True, max_length=self.max_len,\n",
        "            padding='max_length', truncation=True, return_attention_mask=True, return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.label_map[row['label']], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Tạo DataLoader\n",
        "BATCH_SIZE = 32\n",
        "train_loader_balanced = DataLoader(SentimentDataset(df_train_balanced, tokenizer), batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(SentimentDataset(df_test, tokenizer), batch_size=BATCH_SIZE)\n",
        "\n",
        "print(\"\\nSETUP XONG! Sẵn sàng huấn luyện.\")"
      ],
      "metadata": {
        "id": "kRnGGcIfUJjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- HUẤN LUYỆN TRÊN DỮ LIỆU CÂN BẰNG ---\n",
        "print(\"Đang tải lại mô hình PhoBERT sạch (Reset)...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=3)\n",
        "model.to(device)\n",
        "\n",
        "# Cấu hình\n",
        "EPOCHS = 3\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5) # Đã dùng đúng AdamW từ torch\n",
        "total_steps = len(train_loader_balanced) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "print(f\"\\nBẮT ĐẦU HUẤN LUYỆN VỚI DỮ LIỆU CÂN BẰNG (Balanced Training)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_loader_balanced):\n",
        "        # Đẩy vào GPU\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # In tiến độ\n",
        "        if step % 100 == 0 and step > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"  Epoch {epoch+1} | Batch {step}/{len(train_loader_balanced)} | Loss: {loss.item():.4f} | Time: {elapsed:.0f}s\")\n",
        "\n",
        "# --- ĐÁNH GIÁ KẾT QUẢ ---\n",
        "print(\"\\nTrain xong! Đang chấm điểm lại trên tập Test...\")\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# IN BÁO CÁO KẾT QUẢ\n",
        "print(\"\\nBẢNG KẾT QUẢ CHI TIẾT (SAU KHI CÂN BẰNG DỮ LIỆU):\")\n",
        "target_names = ['Negative', 'Neutral', 'Positive']\n",
        "print(classification_report(true_labels, predictions, target_names=target_names, digits=4, zero_division=0))"
      ],
      "metadata": {
        "id": "qcwPDEFDXRpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load lại file đã cân bằng cũ\n",
        "try:\n",
        "    df_check = pd.read_csv('1-VLSP2018-Restaurant-Train-BALANCED.csv')\n",
        "\n",
        "    # Lấy các mẫu Negative được sinh thêm (thường nằm ở cuối file)\n",
        "    # Giả sử 9000 dòng đầu là gốc, sau đó là sinh thêm\n",
        "    generated_samples = df_check.iloc[9300:].sample(10)\n",
        "\n",
        "    print(\"--- KIỂM TRA CHẤT LƯỢNG DỮ LIỆU SINH RA (EDA) ---\")\n",
        "    for idx, row in generated_samples.iterrows():\n",
        "        print(f\"Label: {row['label']}\")\n",
        "        print(f\"Cau: {row['text']}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "except:\n",
        "    print(\"Bạn cần chạy lại bước load file CSV trước.\")"
      ],
      "metadata": {
        "id": "jephW0bAqdxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BƯỚC 1: CÀI ĐẶT THƯ VIỆN DỊCH ---\n",
        "!pip install -q deep-translator\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# --- BƯỚC 2: HÀM DỊCH NGƯỢC (BACK-TRANSLATION) ---\n",
        "def back_translate(sentence):\n",
        "    try:\n",
        "        # 1. Việt -> Anh\n",
        "        en_text = GoogleTranslator(source='vi', target='en').translate(sentence)\n",
        "        # 2. Anh -> Việt\n",
        "        vi_text = GoogleTranslator(source='en', target='vi').translate(en_text)\n",
        "        return vi_text\n",
        "    except:\n",
        "        # Nếu lỗi mạng thì trả về câu gốc\n",
        "        return sentence\n",
        "\n",
        "# Test thử xem nó thông minh thế nào\n",
        "sample = \"Món ăn ở đây không ngon chút nào, thái độ nhân viên lồi lõm.\"\n",
        "print(f\"Gốc: {sample}\")\n",
        "print(f\"Mới: {back_translate(sample)}\")\n",
        "\n",
        "# --- BƯỚC 3: QUY TRÌNH CÂN BẰNG DỮ LIỆU (Kết hợp Oversampling + Back-Translation) ---\n",
        "def balance_with_back_translation(df):\n",
        "    print(\"Đang thực hiện Tăng cường dữ liệu bằng DỊCH NGƯỢC (Chất lượng cao)...\")\n",
        "\n",
        "    # Tách các lớp\n",
        "    df_pos = df[df['label'] == 'positive']\n",
        "    df_neu = df[df['label'] == 'neutral']\n",
        "    df_neg = df[df['label'] == 'negative']\n",
        "\n",
        "    # Lấy số lượng target (Positive)\n",
        "    max_count = len(df_pos)\n",
        "\n",
        "    # Danh sách chứa dữ liệu cuối cùng (Bắt đầu bằng dữ liệu gốc)\n",
        "    final_data = df.to_dict('records')\n",
        "\n",
        "    # --- XỬ LÝ LỚP NEGATIVE (Quan trọng nhất) ---\n",
        "    # Vì Negative rất ít (670 mẫu), ta cần sinh thêm rất nhiều\n",
        "    neg_needed = max_count - len(df_neg)\n",
        "    print(f\"   + Đang sinh thêm {neg_needed} mẫu Negative...\")\n",
        "\n",
        "    neg_samples = df_neg.to_dict('records')\n",
        "    for _ in tqdm(range(neg_needed)):\n",
        "        # Chọn ngẫu nhiên 1 câu mẫu\n",
        "        sample = random.choice(neg_samples)\n",
        "\n",
        "        # Chiến thuật: 50% dùng Dịch ngược (Tạo mới), 50% dùng Copy (An toàn)\n",
        "        # Lý do: Dịch nhiều quá sẽ chậm, kết hợp copy để nhanh hơn\n",
        "        if random.random() < 0.5:\n",
        "            new_text = back_translate(sample['text'])\n",
        "        else:\n",
        "            new_text = sample['text'] # Copy nguyên bản\n",
        "\n",
        "        final_data.append({\n",
        "            'text': new_text,\n",
        "            'aspect': sample['aspect'],\n",
        "            'label': sample['label']\n",
        "        })\n",
        "\n",
        "    # --- XỬ LÝ LỚP NEUTRAL ---\n",
        "    neu_needed = max_count - len(df_neu)\n",
        "    print(f\"   + Đang sinh thêm {neu_needed} mẫu Neutral...\")\n",
        "\n",
        "    neu_samples = df_neu.to_dict('records')\n",
        "    for _ in tqdm(range(neu_needed)):\n",
        "        sample = random.choice(neu_samples)\n",
        "        # Với Neutral, ta dùng Copy nhiều hơn để tiết kiệm thời gian (70% Copy)\n",
        "        if random.random() < 0.3:\n",
        "            new_text = back_translate(sample['text'])\n",
        "        else:\n",
        "            new_text = sample['text']\n",
        "\n",
        "        final_data.append({\n",
        "            'text': new_text,\n",
        "            'aspect': sample['aspect'],\n",
        "            'label': sample['label']\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(final_data)\n",
        "\n",
        "# --- THỰC THI (Lưu ý: Sẽ mất khoảng 10-15 phút vì phải dịch online) ---\n",
        "# Load lại dữ liệu gốc\n",
        "df_train_original = parse_vlsp_data('1-VLSP2018-SA-Restaurant-train.txt')\n",
        "\n",
        "# Chạy cân bằng\n",
        "df_train_bt = balance_with_back_translation(df_train_original)\n",
        "\n",
        "# Lưu file xịn nhất\n",
        "df_train_bt.to_csv('1-VLSP2018-Restaurant-Train-BACKTRANS.csv', index=False)\n",
        "print(\"Đã lưu file chất lượng cao: 1-VLSP2018-Restaurant-Train-BACKTRANS.csv\")"
      ],
      "metadata": {
        "id": "QBffK_NRrfEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForSequenceClassification, get_linear_schedule_with_warmup, AutoTokenizer\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# --- BƯỚC 1: LOAD DỮ LIỆU ĐÃ DỊCH NGƯỢC ---\n",
        "print(\"Đang tải dữ liệu Back-Translation...\")\n",
        "try:\n",
        "    # Đọc file bạn vừa tạo xong\n",
        "    df_train_final = pd.read_csv('1-VLSP2018-Restaurant-Train-BACKTRANS.csv')\n",
        "    print(f\"Đã tải tập TRAIN (Final): {len(df_train_final)} dòng\")\n",
        "    print(\"   Phân bố:\", df_train_final['label'].value_counts().to_dict())\n",
        "except FileNotFoundError:\n",
        "    print(\"LỖI: Không tìm thấy file csv. Hãy kiểm tra lại tên file.\")\n",
        "\n",
        "# Load lại tập Test (nếu chưa có)\n",
        "if 'df_test' not in locals():\n",
        "    # Giả định hàm parse_vlsp_data đã có từ các bước trước\n",
        "    df_test = parse_vlsp_data('3-VLSP2018-SA-Restaurant-test.txt')\n",
        "\n",
        "# --- BƯỚC 2: CHUẨN BỊ DATALOADER ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "# Tạo DataLoader cho tập train mới\n",
        "train_loader_final = DataLoader(SentimentDataset(df_train_final, tokenizer), batch_size=32, shuffle=True)\n",
        "# DataLoader cho tập test (giữ nguyên)\n",
        "test_loader = DataLoader(SentimentDataset(df_test, tokenizer), batch_size=32)\n",
        "\n",
        "# --- BƯỚC 3: KHỞI TẠO MÔ HÌNH (RESET) ---\n",
        "print(\"\\nReset mô hình để học lại từ đầu...\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=3)\n",
        "model.to(device)\n",
        "\n",
        "# --- BƯỚC 4: HUẤN LUYỆN (TRAINING LOOP) ---\n",
        "EPOCHS = 3\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_loader_final) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "print(f\"\\nBẮT ĐẦU HUẤN LUYỆN (PHƯƠNG PHÁP BACK-TRANSLATION)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_loader_final):\n",
        "        # Đẩy vào GPU\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # In tiến độ (Mỗi 100 batch)\n",
        "        if step % 100 == 0 and step > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"  Epoch {epoch+1} | Batch {step}/{len(train_loader_final)} | Loss: {loss.item():.4f} | Time: {elapsed:.0f}s\")\n",
        "\n",
        "# --- BƯỚC 5: ĐÁNH GIÁ KẾT QUẢ CUỐI CÙNG ---\n",
        "print(\"\\nTrain xong! Đang chấm điểm lại trên tập Test...\")\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# IN BÁO CÁO KẾT QUẢ\n",
        "print(\"\\nBẢNG KẾT QUẢ CHI TIẾT (FINAL - BACK TRANSLATION):\")\n",
        "target_names = ['Negative', 'Neutral', 'Positive']\n",
        "print(classification_report(true_labels, predictions, target_names=target_names, digits=4, zero_division=0))"
      ],
      "metadata": {
        "id": "lg7WFtQO4yXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Số liệu từ các lần chạy của bạn (Nhập tay từ các bảng classification_report)\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "# F1-Score của Baseline (Dữ liệu gốc) - Lấy từ ảnh cũ của bạn\n",
        "baseline_f1 = [0.39, 0.49, 0.85]\n",
        "\n",
        "# F1-Score của Final Model (Back-Translation) - Lấy từ kết quả vừa chạy\n",
        "final_f1 = [0.41, 0.57, 0.85]\n",
        "\n",
        "x = np.arange(len(labels))  # Vị trí các nhãn\n",
        "width = 0.35  # Độ rộng cột\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Vẽ 2 cột\n",
        "rects1 = ax.bar(x - width/2, baseline_f1, width, label='Baseline (Imbalanced)', color='#95a5a6')\n",
        "rects2 = ax.bar(x + width/2, final_f1, width, label='Back-Translation (Balanced)', color='#2ecc71')\n",
        "\n",
        "# Trang trí\n",
        "ax.set_ylabel('F1-Score')\n",
        "ax.set_title('HIỆU QUẢ CỦA PHƯƠNG PHÁP TĂNG CƯỜNG DỮ LIỆU (BACK-TRANSLATION)', fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1.1) # Giới hạn trục y\n",
        "\n",
        "# Hàm hiển thị số trên đầu cột\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FjtrScWd-z3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BƯỚC 1: CÀI ĐẶT & KẾT NỐI ---\n",
        "!pip install -q transformers torch scikit-learn\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from google.colab import drive\n",
        "\n",
        "# Kết nối Drive\n",
        "drive.mount('/content/drive')\n",
        "folder_path = '/content/drive/My Drive/ABSA_Project'\n",
        "\n",
        "try:\n",
        "    os.chdir(folder_path)\n",
        "    print(f\"Đang làm việc tại: {os.getcwd()}\")\n",
        "except:\n",
        "    print(\" Kiểm tra lại đường dẫn folder_path!\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Thiết bị: {device}\")\n",
        "\n",
        "# --- BƯỚC 2: CÁC HÀM CẦN THIẾT (ĐÃ SỬA LỖI CÚ PHÁP) ---\n",
        "def parse_vlsp_data(file_name):\n",
        "    lines = []\n",
        "    # Thử đọc utf-16\n",
        "    try:\n",
        "        with open(file_name, 'r', encoding='utf-16') as f:\n",
        "            lines = f.readlines()\n",
        "    except:\n",
        "        # Nếu lỗi thì đọc utf-8\n",
        "        try:\n",
        "            with open(file_name, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "        except:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    dataset = []\n",
        "    review_buffer = []\n",
        "    label_pattern = re.compile(r\"\\{(.*?),\\s*(.*?)\\}\")\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        if \"{\" in line and \"}\" in line and \"#\" in line:\n",
        "            current_review = \" \".join(review_buffer).strip()\n",
        "            matches = label_pattern.findall(line)\n",
        "            for aspect, sentiment in matches:\n",
        "                if sentiment.lower() != 'none':\n",
        "                    dataset.append({'text': current_review, 'aspect': aspect.strip(), 'label': sentiment.strip().lower()})\n",
        "            review_buffer = []\n",
        "        else:\n",
        "            review_buffer.append(re.sub(r\"^[\\_\\-]\\s*\", \"\", line))\n",
        "    return pd.DataFrame(dataset)\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=128):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        input_text = f\"{str(row['aspect'])} {self.tokenizer.sep_token} {str(row['text'])}\"\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            input_text, add_special_tokens=True, max_length=self.max_len,\n",
        "            padding='max_length', truncation=True, return_attention_mask=True, return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.label_map[row['label']], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# --- BƯỚC 3:LOAD DỮ LIỆU & SETUP ---\n",
        "print(\"\\n Đang tải dữ liệu...\")\n",
        "# Load tập Train đã Cân bằng\n",
        "try:\n",
        "    # Ưu tiên dùng file Back-Translation nếu có\n",
        "    if os.path.exists('1-VLSP2018-Restaurant-Train-BACKTRANS.csv'):\n",
        "        df_train = pd.read_csv('1-VLSP2018-Restaurant-Train-BACKTRANS.csv')\n",
        "        print(\"Đã tải file BACKTRANS.\")\n",
        "    else:\n",
        "        df_train = pd.read_csv('1-VLSP2018-Restaurant-Train-BALANCED.csv')\n",
        "        print(\"Đã tải file BALANCED.\")\n",
        "\n",
        "    print(f\"   Tổng số dòng: {len(df_train)}\")\n",
        "except:\n",
        "    print(\" LỖI: Không tìm thấy file csv cân bằng. Bạn kiểm tra lại Drive!\")\n",
        "    df_train = pd.DataFrame(columns=['text', 'aspect', 'label']) # Fallback\n",
        "\n",
        "# Load Test\n",
        "df_test = parse_vlsp_data('3-VLSP2018-SA-Restaurant-test.txt')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "train_loader = DataLoader(SentimentDataset(df_train, tokenizer), batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(SentimentDataset(df_test, tokenizer), batch_size=32)\n",
        "\n",
        "# --- BƯỚC 4: HUẤN LUYỆN (TRAINING) ---\n",
        "print(\"\\nĐang huấn luyện lại mô hình để lưu...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=3)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_loader) * 3 # 3 Epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "for epoch in range(3): # Chạy 3 Epochs\n",
        "    model.train()\n",
        "    print(f\"Epoch {epoch+1}/3... (Đang chạy, vui lòng đợi)\")\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "    print(f\"   -> Xong Epoch {epoch+1}\")\n",
        "\n",
        "# --- BƯỚC 5: LƯU MODEL VÀO DRIVE ---\n",
        "print(\"\\nĐang lưu model vào Drive...\")\n",
        "save_path = '/content/drive/My Drive/ABSA_Project/Model_Final'\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\" ĐÃ LƯU THÀNH CÔNG TẠI: {save_path}\")\n",
        "print(\" Bây giờ bạn có thể sang Sổ tay Demo để chạy App rồi!\")"
      ],
      "metadata": {
        "id": "2ii9s6xQheKE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
